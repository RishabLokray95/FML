{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOME WORK 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rishab Lokray  (UfId- 9357 3447)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2 hidden layer MLP i have chosen a combination of 15 neurons in 1st later and 6 neurons in the 2nd hidden layer\n",
    "\n",
    "I have chosen these values as discussed in class, we need one neuron for each boundary of a shape. In our case there are 15 lines that define the U and F letters. \n",
    "\n",
    "In second layer i have chosen 6 neurons as i have broken down the shapes into multiple rectangles. A total of 6 rectangles is enough to form U and F.\n",
    "\n",
    "The output layer will have one neuron.\n",
    "\n",
    "Roles:\n",
    "1. 1st hidden layer is used to draw decision boundaries.\n",
    "2. 2nd hidden layer is used to combine all these boundaries \n",
    "3. 3rd output layer is used to organise these boundaries into classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, As discussed in class it is possible to get any classification output using just one single hidden layer. \n",
    "\n",
    "This is given by the UNIVERSAL APPROXIMATION THEORAM.  \n",
    "\n",
    "\"The *Universal Approximation Theorem* states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\\mathbb{R}^N$, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.\"\n",
    "\n",
    "\n",
    "\n",
    "Let $\\phi(\\cdot)$ be a non-constant, bounded and monotonic-increasing continuous function. Let $I_{m_0}$ denote the $m_0$-dimensional unit hypercube $[0, 1]^{m_0}$. The space of continuous functions on $I_{m_0}$ is denoted by $C(I_{m_0})$.  Then, given any function $f \\ni C(I_{m_0})$ and $\\epsilon > 0$, there exists an integer $m_1$ and sets of real constants $\\alpha_i, \\beta_i,$ and $w_{ij}$, where $i = 1, \\ldots, m_1$ and $j = 1, \\ldots, m_0$ such that we may define\n",
    "\n",
    "$$F(x_1, \\ldots, x_{m_0}) = \\sum_{i=1}^{m_1} \\alpha_i \\phi\\left( \\sum_{j=1}^{m_0} w_{ij}x_j + b_i\\right)$$\n",
    "\n",
    "as an approximation realization of the function $f(\\cdot)$, that is, \n",
    "\n",
    "$$\\left| F(x_1, \\ldots, x_{m_0}) - f(x_1, \\ldots, x_{m_0}) \\right| < \\epsilon$$\n",
    "\n",
    "for all $x_1, x_2, \\ldots, x_{m_0}$ that like in the input space.\n",
    "\n",
    "Essentially, the Universal Approximation Theorem states that a single hidden layer is sufficient for a multilayer perceptron to compute a uniform $\\epsilon$ approximation to a given training set - provided you have the *right* number of neurons and the *right* activation function. \n",
    "\n",
    "* However, this does not say that a single hidden layer is optimal with regards to learning time, generalization, etc.)\n",
    "\n",
    "* In other words, a **feed-forward MLP with one hidden layer can approximate arbitrarily closely any continuous function**. (Wow!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
